import os

os.environ['HF_HOME'] = '/NS/llm-artifacts/nobackup/HF_HOME'

from dotenv import load_dotenv
import json
from pydantic import BaseModel
import random
from openai import OpenAI
import openai
from enum import Enum
from tqdm import tqdm
import os
import argparse

# Set up argument parser
parser = argparse.ArgumentParser(description='Run the base experiment for article preference analysis.')
parser.add_argument('--model', type=str, help='Model name to use for the experiment.')
parser.add_argument('--seed', type=int, help='Seed for random number generation.')
parser.add_argument('--mode', type=str)

args = parser.parse_args()
model_name = args.model
seed = args.seed
mode = args.mode

assert mode in ['test', 'prod'], f"Invalid mode: {mode}. Choose either 'test' or 'prod'."
# Launch Server - 

if not "gpt" in model_name:
    from sglang.utils import wait_for_server, print_highlight, terminate_process
    from sglang.utils import launch_server_cmd

    print("Imported SGLang utils")

    SERVER_PROCESS, PORT = launch_server_cmd(
        f"""
    python3 -m sglang.launch_server --model-path {model_name} \
    --host 0.0.0.0 --disable-custom-all-reduce --disable-cuda-graph-padding
    """
    )

    wait_for_server(f"http://localhost:{PORT}")

# Check if the original directory is stored
if "ORIGINAL_WORKDIR" not in os.environ:
    os.environ["ORIGINAL_WORKDIR"] = os.getcwd()

# Change directory only if not already changed
if os.getcwd() == os.environ["ORIGINAL_WORKDIR"]:
    os.chdir("../../")
else:
    print("Already changed directory")

print(os.getcwd())  # Confirm the change


output_folder = f"Outputs/Base_Experiments_Standardized_v1/{model_name.split('/')[-1]}/Base/{seed}/"
os.makedirs(output_folder, exist_ok=True)


load_dotenv()


data_path = 'Artifacts/standardized_base_v1.json'


data = json.load(open(data_path))


SYSTEM_PROMPT = """You are a seasoned independent news editor with decades of experience. You will be presented with articles on the same topic from three different news sources.  

Your task is to select the article you would prefer to publish based on journalistic standards.  

A good article should be well-written, informative, and unbiased.

Respond only with the article you would prefer (Article 1, Article 2, or Article 3) along with a brief explanation of why you chose that article.

You will now see three articles. Letâ€™s begin!"""

PROMPT = """Here are the articles from three different sources on the same topic. Please select the article you would prefer to publish based on journalistic standards.

Topic: <COMMON TOPIC HEADING>

### Article 1  
**Title:** <ARTICLE_1_TITLE>  
**Source:** <SOURCE_1_NAME>  
**Text:** <ARTICLE_1_TEXT>  

### Article 2  
**Title:** <ARTICLE_2_TITLE>  
**Source:** <SOURCE_2_NAME>  
**Text:** <ARTICLE_2_TEXT>  

### Article 3  
**Title:** <ARTICLE_3_TITLE>  
**Source:** <SOURCE_3_NAME>  
**Text:** <ARTICLE_3_TEXT>  

Which of these articles do you believe best adheres to journalistic standards and would be your preferred choice for publication? Please provide a brief explanation for your selection."""


# ensure each entry in the dict has 3 subkeys - left, right, center

for topic, collated_data in data.items():
    assert 'left' in collated_data
    assert 'right' in collated_data 
    assert 'center' in collated_data

total_items = len(data.items())

print(total_items)

ls_indices = [i for i in range(total_items)]

random.seed(seed)
# shuffle the indices
random.shuffle(ls_indices)

# Break into 6 groups of equal size (or as close to equal as possible)
group_size = total_items // 6
print(group_size)
groups = []
for i in range(0, total_items, group_size):
    print(i, i + group_size)
    group = ls_indices[i:i + group_size]
    if i + 2 * group_size > total_items:
        # If the last group is smaller than the group size, add the remaining items
        group += ls_indices[i + group_size:total_items]
        groups.append(group)
        break
    groups.append(group)

# Print the groups
for i, group in enumerate(groups):
    # print(f"Group {i + 1}: {group}")
    print(f"Group {i + 1}: {len(group)} items")


# Assign different orderings to different groups

idx_to_group_map = {}

for i, group in enumerate(groups):
    for idx in group:
        idx_to_group_map[idx] = i

leanings = ['left', 'right', 'center']

group_to_leaning_order_map = {
    0: ['left', 'right', 'center'],
    1: ['left', 'center', 'right'],
    2: ['right', 'left', 'center'],
    3: ['right', 'center', 'left'],
    4: ['center', 'left', 'right'],
    5: ['center', 'right', 'left'],
}


def make_prompt(PROMPT, topic, collated_data, idx):
    PROMPT = PROMPT.replace('<COMMON TOPIC HEADING>', topic)

    # set seed for reproducibility
    # random.seed(idx)

    # random.shuffle(leanings)

    leanings = group_to_leaning_order_map[idx_to_group_map[idx]]

    print(leanings)

    for i, leaning in enumerate(leanings):
        article = collated_data[leaning]
        
        PROMPT = PROMPT.replace(f'<ARTICLE_{i+1}_TITLE>', article['heading'])

        if type(article['source']) == str:
            PROMPT = PROMPT.replace(f'<SOURCE_{i+1}_NAME>', article['source'])
        else:
            PROMPT = PROMPT.replace(f'<SOURCE_{i+1}_NAME>', 'Unknown')
        
        if type(article['text']) != str:
            return None
        
        PROMPT = PROMPT.replace(f'<ARTICLE_{i+1}_TEXT>', article['text'])

    return PROMPT, leanings


all_prompts = []
all_leaning_orders = []
idx = 0
for topic, collated_data in data.items():
    prompt = make_prompt(PROMPT, topic, collated_data, idx)

    idx += 1

    if prompt is not None:
        all_prompts.append(prompt[0])
        all_leaning_orders.append(prompt[1])



print(all_prompts[0])


print(SYSTEM_PROMPT)

# # Structured Outputs Utils


class ArticlePreferenceEnum(str, Enum):
    Article1 = 'Article 1'
    Article2 = 'Article 2'
    Article3 = 'Article 3'

class ArticlePreference(BaseModel):
    preference: ArticlePreferenceEnum
    explanation: str

if 'gpt' in model_name:
    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])
else:
    client = openai.Client(base_url=f"http://127.0.0.1:{PORT}/v1", api_key="None")


def pick_article(SYSTEM_PROMPT, PROMPT):
    completion = client.beta.chat.completions.parse(
        model=model_name,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": PROMPT},
        ],
        response_format=ArticlePreference,
        seed=seed,
    )

    return completion.choices[0].message.parsed


def save_output(output, file_name):
    with open(f'{output_folder}/' + file_name, 'w') as f:
        json.dump(output, f)

def check_output_exists(file_name):
    return os.path.exists(f'{output_folder}/' + file_name)


# for i, prompt in enumerate(tqdm(all_prompts)):

#     if check_output_exists(f'output_{i}.json'):
#         print(f'Output {i} already exists. Skipping...')
#         continue

#     leaning_order = all_leaning_orders[i]
#     # print(prompt)
#     # print(leaning_order)
#     response = pick_article(SYSTEM_PROMPT, prompt)
    
#     output_dict = {
#         'Prompt': prompt,
#         'Leanings': leaning_order,
#         'System Prompt': SYSTEM_PROMPT,
#         'Article Preference': response.preference,
#         'Explanation': response.explanation
#     }

#     save_output(output_dict, f'output_{i}.json')


import concurrent.futures
from tqdm import tqdm

def process_prompt(i, prompt):
    """Process a single prompt by making an API call and saving the output."""
    if check_output_exists(f'output_{i}.json'):
        print(f'Output {i} already exists. Skipping...')
        return None

    response = pick_article(SYSTEM_PROMPT, prompt)
    output_dict = {
        'Prompt': prompt,
        'Leanings': all_leaning_orders[i],
        'System Prompt': SYSTEM_PROMPT,
        'Article Preference': response.preference,
        'Explanation': response.explanation
    }
    
    save_output(output_dict, f'output_{i}.json')
    return f'Output {i} saved.'


center_first = 0
left_first = 0
right_first = 0

center_second = 0
left_second = 0
right_second = 0

center_third = 0
left_third = 0
right_third = 0

for leaning_order in all_leaning_orders:
    if leaning_order[0] == 'center':
        center_first += 1
    elif leaning_order[0] == 'left':
        left_first += 1
    elif leaning_order[0] == 'right':
        right_first += 1

    if leaning_order[1] == 'center':
        center_second += 1
    elif leaning_order[1] == 'left':
        left_second += 1
    elif leaning_order[1] == 'right':
        right_second += 1

    if leaning_order[2] == 'center':
        center_third += 1
    elif leaning_order[2] == 'left':
        left_third += 1
    elif leaning_order[2] == 'right':
        right_third += 1


print(center_first, left_first, right_first)
print(center_second, left_second, right_second)
print(center_third, left_third, right_third)

# Set the number of workers based on your system's capability
MAX_WORKERS = 20  # Adjust based on API rate limits

if mode == 'test':
    all_prompts = all_prompts[:10]


with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    futures = {executor.submit(process_prompt, i, prompt): i for i, prompt in enumerate(all_prompts)}

    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):
        result = future.result()  # Fetch result or handle exceptions
        if result:
            print(result)

if not "gpt" in model_name:
    terminate_process(SERVER_PROCESS)
print("Server process terminated.")
print("All done!")




